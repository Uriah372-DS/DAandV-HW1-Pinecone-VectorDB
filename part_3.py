# -*- coding: utf-8 -*-
"""Part 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e5d6NVDuWN0MbRycsIoE5K0NRgB6nnpy
"""

from sentence_transformers import SentenceTransformer
from datasets import load_dataset
from pinecone import Pinecone, ServerlessSpec
from tqdm import tqdm
import cohere
import pinecone
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")

"""### API Keys:"""

from google.colab import userdata
COHERE_API_KEY = userdata.get('COHERE_API_KEY')
PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')

"""## First Element - Embedding Model"""

EMBEDDING_MODEL = 'all-MiniLM-L6-v2'
model = SentenceTransformer(EMBEDDING_MODEL)

def load_and_embed_dataset(
        dataset_name: str,
        split: str,
        model: SentenceTransformer,
        text_field: str
) -> tuple:
    """
    Load a dataset and embedd the text field using a sentence-transformer model
    Args:
        dataset_name: The name of the dataset to load
        split: The split of the dataset to load
        model: The model to use for embedding
        text_field: The field in the dataset that contains the text
        rec_num: The number of records to load and embedd
    Returns:
        tuple: A tuple containing the dataset and the embeddings
    """
    print("Loading and embedding the dataset")

    # Load & embed the dataset
    dataset = load_dataset(dataset_name, split=split)
    embeddings = model.encode(dataset[text_field])

    print("Done!")
    return dataset, embeddings

DATASET_NAME = 'RealTimeData/bbc_news_july_2023'

dataset, embeddings = load_and_embed_dataset(
    dataset_name=DATASET_NAME,
    split='train',
    model=model,
    text_field='content'
)
shape = embeddings.shape

"""## Second Element - Vector Database"""

def create_pinecone_index(
        index_name: str,
        dimension: int,
        metric: str
) -> Pinecone:
    """
    Create a pinecone index if it does not exist
    Args:
        index_name: The name of the index
        dimension: The dimension of the index
        metric: The metric to use for the index
    Returns:
        Pinecone: A pinecone object which can later be used for upserting vectors and connecting to VectorDBs
    """
    print("Creating a Pinecone index...")
    pc = Pinecone(api_key=PINECONE_API_KEY)
    existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]
    if index_name not in existing_indexes:
        pc.create_index(
            name=index_name,
            dimension=dimension,
            # Remember! It is crucial that the metric you will use in your VectorDB will also be a metric your embedding
            # model works well with!
            metric=metric,
            spec=ServerlessSpec(
                cloud="aws",
                region="us-east-1"
            )
        )
    print("Done!")
    return pc

INDEX_NAME = 'bbc-news-july-2023'

# Create the vector database
# We are passing the index_name and the size of our embeddings
pc = create_pinecone_index(index_name=INDEX_NAME, dimension=shape[1], metric="cosine")

def upsert_vectors(
        index: pinecone.data.index.Index,
        embeddings: np.ndarray,
        dataset: dict,
        text_field: str,
        batch_size: int
) -> pinecone.data.index.Index:
    """
    Upsert vectors to a pinecone index
    Args:
        index: The pinecone index object
        embeddings: The embeddings to upsert
        dataset: The dataset containing the metadata
        batch_size: The batch size to use for upserting
    Returns:
        An updated pinecone index
    """
    print("Upserting the embeddings to the Pinecone index...")
    shape = embeddings.shape

    ids = [str(i) for i in range(shape[0])]
    meta = [{text_field: text.encode('utf-8')[:40959].decode('utf-8', 'ignore')} for text in dataset[text_field]]

    # create list of (id, vector, metadata) tuples to be upserted
    to_upsert = list(zip(ids, embeddings, meta))

    for i in tqdm(range(0, shape[0], batch_size)):
        i_end = min(i + batch_size, shape[0])
        index.upsert(vectors=to_upsert[i:i_end])
    return index

# Upsert the embeddings to the Pinecone index
index = pc.Index(INDEX_NAME)
index_upserted = upsert_vectors(index=index, embeddings=embeddings, dataset=dataset, text_field='content', batch_size=128)

index.describe_index_stats()

"""## Third Element - LLM
We will use [Cohere's chat API](https://cohere.com/chat)

## Fourth Element - Query Function
"""

def augment_prompt(
        query: str,
        index: pinecone.data.index.Index,
        model: SentenceTransformer,
        text_field: str
) -> tuple[str, str]:
    """
    Augment the prompt with the top 3 results from the knowledge base
    Args:
        query: The query to augment
        index: The vectorstore object
    Returns:
        str: The augmented prompt
    """
    results = [float(val) for val in list(model.encode(query))]

    # get top 3 results from knowledge base
    query_results = index.query(
        vector=results,
        top_k=5,
        include_values=True,
        include_metadata=True
    )['matches']
    text_matches = [matche['metadata'][text_field] for matche in query_results]

    # get the text from the results
    source_knowledge = "\n\n".join(text_matches)

    # feed into an augmented prompt
    augmented_prompt = f"""Using only the contexts below, answer the query.
    Contexts:
    {source_knowledge}
    If the answer is not included in the source knowledge - say that you don't know.
    Query: {query}"""
    return augmented_prompt, source_knowledge

# QA Model:
co = cohere.Client(api_key=COHERE_API_KEY)

# Queries for the LLM:
queries = ["What was the score in the Wimbledon’s 2023 final between Carlos Alcaraz and Novak Djokovic?",
           "What happened to Esther Wang?",
           "What happened to Dutch racing driver Dilano Van’t Hoff at the formula regional European championship?"]

def add_underline(text: str = ""):
    """ Adds underline to text when printed using `print`. """
    return f"\033[4m{text}\033[0m"

for i, query in enumerate(queries):
    print("\n" + add_underline(text=f"Query {i+1}") + f": {query}")
    response = co.chat(
            model='command-r-plus',
            message=query,
    )
    print(add_underline(text="Standard QA Model Answer") + ":")
    print(response.text)

    augmented_prompt, source_knowledge = augment_prompt(query, model=model, index=index, text_field='content')
    response = co.chat(
            model='command-r-plus',
            message=augmented_prompt
    )
    print(add_underline(text="RAG Pipeline Answer") + ":")
    print(response.text)